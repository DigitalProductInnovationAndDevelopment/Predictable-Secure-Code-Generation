# Enterprise Code Generation Platform

A clean, enterprise-grade architecture for a Python system with three first-class services:

- **S1 â€” Code Generation from Requirements**
- **S2 â€” Metadata Generation from Code**
- **S3 â€” Validation** (syntax â†’ tests â†’ AI logic checks)

## ğŸ—ï¸ Architecture

Built using **Hexagonal (Ports & Adapters)** architecture with:

- **Provider pattern** for language-specific behavior
- **Pipeline pattern** for multi-stage validation
- **Command pattern** for consistent execution
- **Registry pattern** for dynamic provider discovery
- **Strategy pattern** for pluggable AI backends

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
# or with poetry
poetry install
```

### 2. Configure Environment

```bash
cp .env.example .env
# Edit .env with your OpenAI/Azure credentials
```

### 3. Check Platform Status

```bash
python -m platform.interfaces.cli.main status
```

### 4. Generate Code

```bash
# Generate Python code from requirements
python -m platform.interfaces.cli.main s1-generate \
  examples/requirements/sample_calculator.json \
  python \
  --output ./generated \
  --context examples/context/python_context.json
```

### 5. Extract Metadata

```bash
# Extract metadata from generated code
python -m platform.interfaces.cli.main s2-metadata \
  ./generated \
  --output metadata.json
```

### 6. Validate Code

```bash
# Run full validation pipeline
python -m platform.interfaces.cli.main s3-validate \
  ./generated \
  --requirements examples/requirements/sample_calculator.json \
  --metadata metadata.json \
  --ai-check \
  --output validation_report.json
```

## ğŸ“ Project Structure

```
src/platform/
â”œâ”€â”€ app/                        # Application layer (use-cases, pipelines)
â”‚   â”œâ”€â”€ s1_codegen/            # S1 - Code Generation service
â”‚   â”œâ”€â”€ s2_metadata/           # S2 - Metadata extraction service
â”‚   â””â”€â”€ s3_validation/         # S3 - Validation service
â”‚
â”œâ”€â”€ domain/                     # Pure domain logic
â”‚   â”œâ”€â”€ models/                # Domain models (Pydantic)
â”‚   â”œâ”€â”€ errors.py              # Domain errors
â”‚   â””â”€â”€ policies.py            # Business policies
â”‚
â”œâ”€â”€ ports/                      # Interfaces (contracts)
â”‚   â”œâ”€â”€ ai.py                  # LLM & embeddings clients
â”‚   â”œâ”€â”€ fs.py                  # File system operations
â”‚   â”œâ”€â”€ providers.py           # Language providers
â”‚   â”œâ”€â”€ runners.py             # Test runners & sandbox
â”‚   â””â”€â”€ observability.py       # Logging, metrics, tracing
â”‚
â”œâ”€â”€ adapters/                   # External system implementations
â”‚   â”œâ”€â”€ ai/                    # OpenAI, Azure OpenAI
â”‚   â”œâ”€â”€ fs/                    # Local file system
â”‚   â”œâ”€â”€ runners/               # pytest, subprocess sandbox
â”‚   â””â”€â”€ providers/             # Python, TypeScript, etc.
â”‚
â”œâ”€â”€ kernel/                     # Cross-cutting infrastructure
â”‚   â”œâ”€â”€ config.py              # Pydantic settings
â”‚   â”œâ”€â”€ di.py                  # Dependency injection
â”‚   â”œâ”€â”€ registry.py            # Provider discovery
â”‚   â””â”€â”€ logging.py             # Structured logging
â”‚
â””â”€â”€ interfaces/                 # CLI & API boundaries
    â”œâ”€â”€ cli/                   # Typer CLI
    â””â”€â”€ api/                   # FastAPI (future)
```

### Legacy Structure (v1)

```
â”œâ”€â”€ function_app.py                # Azure Functions entry
â”œâ”€â”€ config.py                      # High-level config (paths, filters, limits; uses .env)
â”œâ”€â”€ requirements.txt               # Python deps for the v1 toolset
â”œâ”€â”€ quick_setup.py                 # Helper to install minimal deps
â”œâ”€â”€ install_dependencies.py        # Installs broader toolchain (linters, pytest, etc.)
â”œâ”€â”€ check_env.py                   # Verifies env vars (.env) and prints guidance
â”œâ”€â”€ env.template                   # Sample env vars (copy to .env and edit)
â”œâ”€â”€ input/                         # Place your input files here (e.g., CSV requirements)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ HandlePython/              # v1: Python-specific S1/S2/S3 modules
â”‚   â”‚   â”œâ”€â”€ AIBrain/               # Azure OpenAI wrapper & CLI
â”‚   â”‚   â”œâ”€â”€ CheckCodeRequirements/ # CSV diff utilities
â”‚   â”‚   â”œâ”€â”€ GenerateCodeFromRequirements/  # S1: plan, generate, integrate
â”‚   â”‚   â”œâ”€â”€ GenerateMetadataFromCode/      # S2: AST-based metadata
â”‚   â”‚   â””â”€â”€ ValidationUnit/                # S3: syntax / tests / AI checks
â”‚   â”œâ”€â”€ HandleGeneric/             # v1: language-agnostic base + providers
â”‚   â”‚   â”œâ”€â”€ core/                  # registry, detection, generic generator/validator
â”‚   â”‚   â””â”€â”€ providers/             # python/typescript/javaâ€¦ providers
â”‚   â””â”€â”€ HandleGeneric v2/          # v2: layered "platform" (Domain/Adapters/App/Interfaces)
â”‚       â”œâ”€â”€ pyproject.toml         # can be installed as a package
â”‚       â””â”€â”€ src/platform/          # see "v2 platform" above
â”œâ”€â”€ test_ai.py                     # Smoke tests for the AI layer (v1)
â”œâ”€â”€ test_ai_cli.py                 # Smoke tests for AI CLI (v1)
â””â”€â”€ src/validate_python_code.py    # Standalone syntax check helper
```

## ğŸ”§ Services Overview

### S1 - Code Generation

Generates production-ready code from requirements:

```bash
# Basic usage
handle s1-generate requirements.json python

# With context and custom output
handle s1-generate requirements.json python \
  --output ./src \
  --context context.json \
  --dry-run
```

**Features:**

- Multi-language support (Python, TypeScript, Java)
- Prompt templating with context
- Automatic formatting (Black, isort)
- Syntax validation
- Cost tracking

### S2 - Metadata Extraction

Extracts structured metadata from codebases:

```bash
# Extract metadata from project
handle s2-metadata ./my-project --output metadata.json

# Filter by language
handle s2-metadata ./my-project \
  --languages python,typescript \
  --exclude node_modules,venv
```

**Features:**

- AST-based parsing
- Multi-language support
- Function/class extraction
- Import analysis
- LOC counting

### S3 - Validation Pipeline

Comprehensive validation in stages:

```bash
# Full validation pipeline
handle s3-validate ./project \
  --requirements requirements.json \
  --ai-check \
  --output report.json

# Syntax only
handle s3-validate ./project --no-tests
```

**Pipeline Stages:**

1. **Syntax**: AST parsing, linting
2. **Tests**: pytest, jest, etc.
3. **AI Logic**: Requirements consistency check

## ğŸ§© Language Providers

The platform uses providers for language-specific operations:

### Python Provider

- **Code Generation**: PEP 8, type hints, docstrings
- **Metadata**: AST parsing for functions/classes
- **Syntax**: ast.parse validation
- **Tests**: pytest runner

### TypeScript Provider (Planned)

- **Code Generation**: TSDoc, interfaces
- **Metadata**: TS compiler API
- **Syntax**: tsc --noEmit
- **Tests**: jest runner

## âš™ï¸ Configuration

Environment variables in `.env`:

```bash
# AI Configuration
LLM_BACKEND=openai              # openai | azure
OPENAI_API_KEY=sk-...
MODEL=gpt-4
TEMPERATURE=0.0

# Limits
MAX_TOKENS=4000
MAX_REQUESTS_PER_HOUR=100
DRY_RUN=false

# File Processing
MAX_FILE_SIZE_MB=10
IGNORED_DIRECTORIES=.git,node_modules,dist

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json
```

## ğŸ” CLI Commands

### Core Commands

- `handle s1-generate` - Generate code from requirements
- `handle s2-metadata` - Extract metadata from code
- `handle s3-validate` - Validate code (syntax â†’ tests â†’ AI)
- `handle status` - Show platform status
- `handle version` - Show version info

### Examples

```bash
# Generate Python calculator
handle s1-generate examples/requirements/calculator.json python

# Extract metadata with language filter
handle s2-metadata ./src --languages python --output meta.json

# Validate with AI logic check
handle s3-validate ./src --requirements req.json --ai-check

# Check what providers are available
handle status
```

## ğŸ§ª Testing

```bash
# Run unit tests
pytest tests/unit/

# Run integration tests
pytest tests/integration/

# Run end-to-end tests
pytest tests/e2e/

# With coverage
pytest --cov=platform tests/
```

## ğŸš¢ Deployment

### Docker

```bash
# Build image
docker build -t platform:latest .

# Run CLI
docker run -it platform:latest handle status

# Run API (future)
docker run -p 8000:8000 platform:latest uvicorn platform.interfaces.api.app:app
```

### CI/CD

GitHub Actions workflow included for:

- Linting (ruff, mypy)
- Testing (pytest)
- Security (bandit)
- Docker builds

## ğŸ”® Roadmap

### Phase 1 - MVP âœ…

- [x] Python providers (all 3 services)
- [x] CLI interface with Typer
- [x] OpenAI/Azure OpenAI support
- [x] Basic validation pipeline

### Phase 2 - Extensibility

- [ ] TypeScript providers
- [ ] Java providers
- [ ] Plugin system
- [ ] Web dashboard

### Phase 3 - Production

- [ ] FastAPI REST API
- [ ] Authentication & authorization
- [ ] Rate limiting & quotas
- [ ] Metrics & monitoring
- [ ] Multi-tenant support

### Phase 4 - Advanced

- [ ] Custom model fine-tuning
- [ ] Advanced static analysis
- [ ] Integration with IDEs
- [ ] Collaborative features

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

### Adding Language Providers

To add a new language:

1. Create provider directory: `adapters/providers/mylang/`
2. Implement the three providers:
   - `codegen_provider.py`
   - `metadata_provider.py`
   - `syntax_validator.py`
3. Register in `kernel/di.py`
4. Add tests in `tests/unit/providers/mylang/`

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- OpenAI for GPT models
- Pydantic for data validation
- Typer for CLI framework
- FastAPI for future API layer
- Rich for beautiful CLI output
